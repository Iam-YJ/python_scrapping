## python day9 & 10 0421 & 0422

## 4.1 introduction to Flask
Flask를 패키지에서 찾아서 설치한다.
```python main.py
from flask import Flask

app = Flask("superscrapper")

@app.route("/")
def home():
    return "hello"

@app.route("/contact")
def contact():
    return "contact me!"


app.run(host = "0.0.0.0") #repl.it을 이용해서 서버를 구축했다.
```
@(데코레이터)가 바로 아래 있는 함수를 찾아서 실행해준다. 

## 4.2 dynamic URLs and templates
```python main.py
from flask import Flask, render_template

app = Flask("superscrapper")

@app.route("/")
def home():
    return render_template("potato.html")

@app.route("/<username>") #dynamic url을 이용하는 예시다
def contact(username):
    return f"hello your name is {username} !"


app.run(host = "0.0.0.0") #repl.it을 이용해서 서버를 구축했다.
```

``` html home.html 
<!DOCTYPE html>
<html>
    <head>
        <title>Job Search</title>
    </head>
    <body>
    <h1> Job Search </h1>
    <input placeholder='what job do you want?' required/>
    <button>SEARCH</button>
    </body>
</html>
```

## 4.3 forms and query arguments & 4.4 scrapper intrgration

```python main.py
from flask import Flask, render_template, request, redirect

app = Flask("superscrapper")

@app.route("/")
def home():
    return render_template("home.html")

@app.route("/report") #dynamic url을 이용하는 예시다
def report():
    word = request.args.get{'word'}
    if word():
        word = word.lower()
    else:
        return redirect("/")
    return render_template("report.html", searchingBy = word)

app.run(host = "0.0.0.0") #repl.it을 이용해서 서버를 구축했다.
```

``` html home.html 
<!DOCTYPE html>
<html>
    <head>
        <title>Job Search</title>
    </head>
    <body>
    <h1> Job Search </h1>
    <form action="/report" method="get">
    <input placeholder='search for a job' required name="word"/>
    <button>SEARCH</button>
    </form>
    </body>
</html>
```

``` html report.html 
<!DOCTYPE html>
<html>
    <head>
        <title>Job Search</title>
    </head>
    <body>
    <h1> Search Results </h1>
    <h3>You are looking for {{searchingBy}}</h3>
    </body>
</html>
```
플라스크는 html을 렌더링하고 {{}}에 내가 준 변수를 넣었다.
그래서 사용자는 우리가 찾는 word로 보게된다.

# 4.4-2 
전에 만든 scrapper + flask로 만든 페이지
BeautifulSoup4, request 패키지 확장.

* scrapper.py
  ```python
  # step1. get the page 
  # step2. make the requests
  # step3. extract the jobs
  import requests
  from bs4 import BeautifulSoup


  def get_last_page(url):
    result = requests.get(url)
    soup = BeautifulSoup(result.text, "html.parser")
    pages = soup.find("div",{"class":"s-pagination"}).find_all("a")
    last_page = pages[-2].get_text(strip=True)
    return int(last_page)


  def extract_job(html):
    title = html.find("h2").find("a")["title"]
    company, location = html.find("h3").find_all("span",recursive=False)
    company = company.get_text(strip=True)
    location = location.get_text(strip=True).strip("-").strip(" \r").strip("\n")
    job_id = html['data-jobid']
    #print(company.get_text(strip=True), location.get_text(strip=True))
    return {'title':title,'company':company,'location':location,"apply_link":f"https://stackoverflow.com/jobs/{job_id}"}


  def extract_jobs(last_page,url):
    jobs = []
    for page in range(last_page):
      print(f"Scrapping SO: Page : {page}")
      result = requests.get(f"{url}&pg={page+1}")
      soup = BeautifulSoup(result.text,"html.parser")
      results = soup.find_all("div",{"class":"-job"})
      for result in results:
        job = extract_job(result)
        jobs.append(job)
    return jobs


  def get_jobs(word):
    url = f"https://www.stackoverflow.com/jobs?q={word}&sort=i"
    last_page = get_last_page(url)
    jobs = extract_jobs(last_page, url)
    return jobs
    ```

    URL로 선언되어 있어 바꿀 수 없던 것은 main의 word로 받아와 사용할 수 있도록 조작함.
  
  * main.py
    ```python
    from flask import Flask, render_template, request, redirect
    from scarpper import get_jobs

    app = Flask("SuperScrapper_YJ")

    @app.route("/")
    def home():
      return render_template("potato.html")

    @app.route("/report")
    def report():
      word = request.args.get('word')
      if word:
        word = word.lower()
        jobs = get_jobs(word)
        print(jobs)
      else:
        return redirect("/")
      return render_template("report.html",searchingBy=word)

    app.run(host="0.0.0.0")
    ```

## 4.5 faster scrapper
* main.py
  ```python
  from flask import Flask, render_template, request, redirect
  from scrapper import get_jobs

  app = Flask("SuperScrapper_YJ")

  db = {}

  @app.route("/")
  def home():
    return render_template("potato.html")

  @app.route("/report")
  def report():
    word = request.args.get('word')
    if word:
      word = word.lower()
      fromDb = db.get(word)
      if fromDb:
        jobs= fromDb    
      else:
        jobs = get_jobs(word)
        db[word] = jobs
    else:
      return redirect("/")
    return render_template("report.html",searchingBy=word, resultsNumber=len(jobs))

  app.run(host="0.0.0.0")
  ```
main에서 fake DB를 만들었다. fake db를 함수 밖에 만들어서 페이지가 새로고침되어도 다시 작업하지 않도록한다(다시 db에 정보를 collect하는 작업을 반복하지 않도록). 

* report.html
  ```html
    <!DOCTYPE html>
  <html>
    <head>
      <title Job Search></title>
    </head>
    <body>
      <h1>Search Results</h1>
      <h3>Found{{resultsNumber}} results for: {{searchingBy}}  </h3>
    </body>
  </html>
  ```
  report.html도 원하는 정보(예 python)가 잘 나왔는지 확인할 수 있도록 조금 수정한다.

## 4.6 rendering jobs!
html과 css를 이용해 페이지에 정보가 예쁘게 나올 수 있도록 조정했다.

* report.html
  ```html
  <!DOCTYPE html>
  <html>
    <head>
      <title Job Search></title>
      <style>
        section{
          display:grid;
          gap:20px;
          grid-template-columns: repeat(4,1fr);
        }
        </style>

    </head>
    <body>
      <h1>Search Results</h1>
      <h3>Found{{resultsNumber}} results for: {{searchingBy}}  </h3>
      <section>
      <h4>Title</h4>
      <h4>Company</h4>
      <h4>Locaion</h4>
      <h4>Link</h4>
      {% for job in jobs %}
        <span>{{job.title}}</span>
        <span>{{job.company}}</span>
        <span>{{job.location}}</span>
        <a href="{{job.link}}" target="_blank">Apply</a>
      {% endfor %}
      <section>
    </body>
  </html>
    ```

* main.py
```python
from flask import Flask, render_template, request, redirect
from scrapper import get_jobs

app = Flask("SuperScrapper_YJ")

db = {}

@app.route("/")
def home():
return render_template("potato.html")

@app.route("/report")
def report():
word = request.args.get('word')
if word:
    word = word.lower()
    fromDb = db.get(word)
    if fromDb:
    jobs= fromDb    
    else:
    jobs = get_jobs(word)
    db[word] = jobs
else:
    return redirect("/")
return render_template("report.html",searchingBy=word, resultsNumber=len(jobs), jobs=jobs)

app.run(host="0.0.0.0")
  ```

main.py 아래쪽에 쓰인 jobs=jobs를 통해서 report.html에서 정보를 가져올 수 있게한다. {% 반복문 %} {{job.title}} 등으로 정보를 가져온다.

* scrapper.py
  ``` python
  # step1. get the page 
  # step2. make the requests
  # step3. extract the jobs
  import requests
  from bs4 import BeautifulSoup


  def get_last_page(url):
    result = requests.get(url)
    soup = BeautifulSoup(result.text, "html.parser")
    pages = soup.find("div",{"class":"s-pagination"}).find_all("a")
    last_page = pages[-2].get_text(strip=True)
    return int(last_page)


  def extract_job(html):
    title = html.find("h2").find("a")["title"]
    company, location = html.find("h3").find_all("span",recursive=False)
    company = company.get_text(strip=True)
    location = location.get_text(strip=True).strip("-").strip(" \r").strip("\n")
    job_id = html['data-jobid']
    #print(company.get_text(strip=True), location.get_text(strip=True))
    return {'title':title,'company':company,'location':location,"apply_link":f"https://stackoverflow.com/jobs/{job_id}"}


  def extract_jobs(last_page,url):
    jobs = []
    for page in range(last_page):
      print(f"Scrapping SO: Page : {page}")
      result = requests.get(f"{url}&pg={page+1}")
      soup = BeautifulSoup(result.text,"html.parser")
      results = soup.find_all("div",{"class":"-job"})
      for result in results:
        job = extract_job(result)
        jobs.append(job)
    return jobs


  def get_jobs(word):
    url = f"https://www.stackoverflow.com/jobs?q={word}&sort=i"
    last_page = get_last_page(url)
    jobs = extract_jobs(last_page, url)
    return jobs
    ```

